{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BugPersonality/ML/blob/main/Linear_Regression_lab_3_1_3_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkTpaRRWZNc3"
      },
      "source": [
        "# Линейная регрессия\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro-ttLpiZNdC"
      },
      "source": [
        "Линейные методы предполагают, что между признаками объекта и целевой переменной существует линейная зависимость, то есть:\n",
        "$$ \\hat{y} = w_1 x_1 + w_2 x_2 + ... + w_k x_k + b,$$\n",
        "где $\\hat{y}$ - целевая переменная (что мы хотим предсказать), $x_i$ - i-ый признак объекта $x$, $w_i$ - вес $i$-го признака, $b$ - bias (смещение, свободный член).\n",
        "\n",
        "В задаче линейной регрессии $\\hat{y}$ - это действительное число.\n",
        "\n",
        "Часто для упрощения записи вводят дополнительный фиктивный признак $x_0$, который всегда равен 1, тогда bias - вес этого признака. В этом случае формула может быть записана как скалярное произведение:\n",
        "$$ \\hat{y} = <w, x> $$\n",
        "\n",
        "В матричной форме формулу можно переписать следующим образом:\n",
        "$$ \\hat{y} = Xw,$$\n",
        "$\\hat{y}$ - вектор значений целевой переменной размера $n$, $X$ - матрица значений признаков объектов размера $n \\times k$, w - вектор весов размера $k$. То есть в наших данных имеется $n$ объектов, каждый их которых описан $k$ признаками.\n",
        "\n",
        "Таким образом, в матричной форме модель задаётся следующим образом:\n",
        "$$ y = Xw + \\epsilon$$ \n",
        "\n",
        "Важно отметить, что параметрами этой модели являются веса $w$. Когда говорят об обучении какого-либо алгоритма машинного обучения, как правило, имеют в виду настройку весов, т.е. параметров модели.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixnX1G4yDVcG"
      },
      "source": [
        "На практике $\\hat{y} $ может отличается от реальных значений, которые принимает целевая переменная $y$. Разницу между реальным значением и предсказанным, обозначим как $\\epsilon$ - вектор значений случайной переменной, соответствующая случайной, непрогнозируемой ошибке модели. Ограничения, которые накладываются на эту модель:\n",
        "* математическое ожидание случайных ошибок $\\epsilon$ равно нулю,\n",
        "* дисперсия случайных ошибок одинакова и конечна,\n",
        "* случайные ошибки не скоррелированы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJyN9MkMDTod"
      },
      "source": [
        "Один из способов вычислить значения параметров модели, давно знаком - это наименьших квадратов, который минимизирует среднеквадратичную ошибку между реальным значением зависимой переменной и прогнозом, выданным моделью. Решение по методу наименьших квадратов дает:\n",
        "$$ w = (X^TX)^{-1}X^TY $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5HVNycSZNc5"
      },
      "source": [
        "Загрузим необходимые библиотеки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Lrhz2sFvZNc7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFQqRj1lZNdg"
      },
      "source": [
        "## Оценка результатов\n",
        "\n",
        "Чтобы оценить качество работы алгоритма нам необходимо применяют разные метрики. Наиболее частые метрики средневадратичная и средняя абсолютная ошибки. Вычислим эти метрики на обучающей и на тестовой выборках. \n",
        "\n",
        " * *mean_absolute_error* - средняя абсолютная ошибка $|y_i - \\hat{y}_i|$\n",
        " * *mean_squared_error* - средняя квадратичная ошибка $(y_i - \\hat{y}_i)^2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sEDF-oXRsVw"
      },
      "source": [
        "## Задание 3.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aStEfJYSZNdE"
      },
      "source": [
        "Пример 1. Сгенерируем искусственные данные. Сначала поработаем с простейшим одномерным случаем, когда у нас значение $y$ будет зависеть только от одного значения $x$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "d3tzSTkrZNdF"
      },
      "outputs": [],
      "source": [
        "def generate_data(n_points=20):\n",
        "  \"\"\"\n",
        "    Принимает на вход n_points точек \n",
        "    Возвращает данные для обучения и теста\n",
        "  \"\"\"\n",
        "  X = np.linspace(-5, 5, n_points)\n",
        "  y = 10 * X - 7\n",
        "\n",
        "  X_train = X[0::2].reshape(-1, 1)\n",
        "  y_train = y[0::2] + np.random.randn(int(n_points/2)) * 10\n",
        "\n",
        "  X_test = X[1::2].reshape(-1, 1)\n",
        "  y_test = y[1::2] + np.random.randn(int(n_points/2)) * 10\n",
        "\n",
        "  print(f'Generated {len(X_train)} train samples and {len(X_test)} test samples')\n",
        "  return X, X_train, y_train, X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "bnYHO2TjZNdJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 50 train samples and 50 test samples\n"
          ]
        }
      ],
      "source": [
        "X, X_train, y_train, X_test, y_test = generate_data(100)\n",
        "\n",
        "xtemp = np.array([])\n",
        "onesA = np.ones(len(X_train))\n",
        "\n",
        "for arr in X_train:\n",
        "    xtemp = np.append(xtemp, arr[0])\n",
        "\n",
        "A = np.stack((onesA, xtemp), axis = -1)\n",
        "\n",
        "# print(X)\n",
        "# print(A)\n",
        "# print(y_train) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "q0l76cbaKzYg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My solution\n",
            "w = 9.655433975429814 b = -8.662030223738412\n",
            "Mean Absolute Error: [7.59155312]\n",
            "Mean Squared Error: [83.32197929]\n"
          ]
        }
      ],
      "source": [
        "### Реализуйте настройку w и b с помощью рассмотренного выше метода наименьших квадратов.\n",
        "### Найдите значения метрик MSE и MAE. Сравните с результатами из sklearn\n",
        "\n",
        "wb = np.dot(np.dot(np.linalg.inv(np.dot(A.transpose(), A)), A.transpose()), y_train)\n",
        "y_pred = np.dot(X_test, wb[1]) + wb[0]\n",
        "\n",
        "mean_absolute_error_test = 0\n",
        "mean_squared_error_test = 0\n",
        "\n",
        "for index in range(len(y_pred)):\n",
        "    mean_absolute_error_test += abs(y_test[index] - y_pred[index])\n",
        "    mean_squared_error_test += pow((y_pred[index] - y_test[index]), 2)\n",
        "\n",
        "mean_absolute_error_test = mean_absolute_error_test / len(y_pred)\n",
        "mean_squared_error_test = mean_squared_error_test / len(y_pred)\n",
        "\n",
        "print(\"My solution\") \n",
        "print(\"w =\", wb[1], \"b =\", wb[0])\n",
        "print('Mean Absolute Error:', mean_absolute_error_test)\n",
        "print('Mean Squared Error:', mean_squared_error_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sklearn\n",
            "w = 9.65543397542981 b = -8.662030223738412\n",
            "Mean Absolute Error: 7.591553117211241\n",
            "Mean Squared Error: 83.32197928861292\n"
          ]
        }
      ],
      "source": [
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train, y_train)\n",
        "y_pred_regressor = regressor.predict(X_test)\n",
        "print(\"Sklearn\") \n",
        "print(\"w =\", regressor.coef_[0], \"b =\", regressor.intercept_)\n",
        "print('Mean Absolute Error:', mean_absolute_error(y_test, y_pred_regressor))\n",
        "print('Mean Squared Error:', mean_squared_error(y_test, y_pred_regressor))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luGJoQXHRzu7"
      },
      "source": [
        "## Задание 3.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKKQZDzWMv-j"
      },
      "source": [
        "Пример 2. Не всегда в задаче регрессии в качестве решения выступает прямая, как в предыдущем случае. Рассмотрим ещё один пример, в котором у объектов всё ещё один признак. Но теперь мы будм брать случайную точку на синусоиде и добавлять к ней шум — таким образом получим целевую переменную, признаком в этом случае будет координата $x$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "aULennJThZxH"
      },
      "outputs": [],
      "source": [
        "def generate_wave_set(n_support=1000, n_train=25, std=0.3):\n",
        "    data = {}\n",
        "    # выберем некоторое количество точек из промежутка от 0 до 2*pi\n",
        "    data['support'] = np.linspace(0, 2*np.pi, num=n_support)\n",
        "    # для каждой посчитаем значение sin(x) + 1\n",
        "    # это будет ground truth\n",
        "    data['values'] = np.sin(data['support']) + 1\n",
        "    # из support посемплируем некоторое количество точек с возвратом, это будут признаки\n",
        "    data['x_train'] = np.sort(np.random.choice(data['support'], size=n_train, replace=True))\n",
        "    # опять посчитаем sin(x) + 1 и добавим шум, получим целевую переменную\n",
        "    data['y_train'] = np.sin(data['x_train']) + 1 + np.random.normal(0, std, size=data['x_train'].shape[0])\n",
        "    return data\n",
        "\n",
        "data = generate_wave_set(1000, 250)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "3CBnODv1QYn_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "w = -0.3184396961165823 b = 2.0022318694993446\n",
            "Mean Absolute Error: 0.4332387106200941\n",
            "Mean Squared Error: 0.27974925587123217\n"
          ]
        }
      ],
      "source": [
        "### попробуйте реализовать настройку w и b с помощью рассмотренного выше метода наименьших квадратов.\n",
        "### Найдите значения метрик MSE и MAE\n",
        "\n",
        "xtemp_2 = np.array([])\n",
        "onesA_2 = np.ones(len(data['x_train']))\n",
        "\n",
        "for arr in data['x_train']:\n",
        "    xtemp_2 = np.append(xtemp_2, arr)\n",
        "\n",
        "A2 = np.stack((onesA_2, xtemp_2), axis = -1)\n",
        "\n",
        "wb = np.dot(np.dot(np.linalg.inv(np.dot(A2.transpose(), A)), A2.transpose()), data['y_train'])\n",
        "\n",
        "y_pred = np.dot(data['x_train'], wb[1]) + wb[0]\n",
        "\n",
        "\n",
        "mean_absolute_error_test = 0\n",
        "mean_squared_error_test = 0\n",
        "\n",
        "for index in range(len(y_pred)):\n",
        "    mean_absolute_error_test += abs(data['y_train'][index] - y_pred[index])\n",
        "    mean_squared_error_test += pow((y_pred[index] - data['y_train'][index]), 2)\n",
        "\n",
        "mean_absolute_error_test = mean_absolute_error_test / len(y_pred)\n",
        "mean_squared_error_test = mean_squared_error_test / len(y_pred)\n",
        "\n",
        "print(\"w =\", wb[1], \"b =\", wb[0])\n",
        "print('Mean Absolute Error:', mean_absolute_error_test)\n",
        "print('Mean Squared Error:', mean_squared_error_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9iBgsGVQyOD"
      },
      "source": [
        "Конечно, такое решение нас вряд ли может устроить. Нужно применить полинимиальную регрессию. Идея здесь такая. Каждый признак в исходную формулу может входить не только в первой степени, но и во второй, в третьей и так далее. То есть для случая, когда у нас только один признак:\n",
        "$$ \\hat{y} = w_1 x_1 + w_2 x_1^2 + ... + w_k x_1^k + b,$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95m0fULWR4m5"
      },
      "source": [
        "## Задание 3.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyzS3rlzSuDU"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "### Реализуйте полиномиальную регрессию. Сделайте визуализацию для полиномов разных степеней. \n",
        "### Полином какой степени подходит больше других? Почему?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feMIFS82ZNdn"
      },
      "source": [
        "# Реальный датасет"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NVLc80wZNdn"
      },
      "source": [
        "Возьмём реальный набор данных Boston из sklearn.datasets. Этот датасет описывает средние цены на недвижимость в районах Бостона в тысячах долларов.\n",
        "\n",
        "Примеры признаков объектов недвижимости: количество преступлений на душу населения, процент старых домов в районе, количество учеников на одного учителя и т.д. Обратите внимание на то, что данные уже оцифрованы там, где изначально признаки были качественными."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMC-plFGZNdo"
      },
      "source": [
        "Загрузим датасет, выведем информацию"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZN6sApHcZNdo"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_boston\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBgnP7HcZNdq"
      },
      "outputs": [],
      "source": [
        "house_data = load_boston()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjL3jxlqgHWs"
      },
      "source": [
        "## Задание 3.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbTQGLKagJN5"
      },
      "outputs": [],
      "source": [
        "### оставьте в наборе данных только 7 наиболее значимых признаков\n",
        "### настройте параметры линейной регрессии и сравните метрики качества (MSE и MAE) для полного датасета и усечённого"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Linear Regression lab 3.1-3.4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}